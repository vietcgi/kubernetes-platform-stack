# Cilium Helm Chart Values
# Configured for BGP, kube-proxy replacement, and native LoadBalancer

# Cilium version (v1.18.3 stable - tested with KIND)
image:
  repository: quay.io/cilium/cilium
  tag: v1.18.3
  pullPolicy: IfNotPresent

# IPAM configuration
ipam:
  mode: kubernetes

# Enable BGP Control Plane for LoadBalancer IP advertisement
bgp:
  enabled: true
  announce:
    loadbalancerIP: true
    podCIDR: true

# BGP Configuration (peer settings)
bgpControlPlane:
  enabled: true
  secretsNamespace:
    create: false
    name: kube-system

# BGP Cluster Configuration (disabled by default, enable for production BGP setups)
bgpClusterConfig:
  enabled: false
  nodeSelector:
    bgp-node: "true"
  localASN: 65000
  exportPodCIDR: true
  routerID: "192.168.1.1"

# BGP Neighbors (production BGP peers)
bgpNeighbors:
  enabled: false

# BGP Advertisements
bgpAdvertisements:
  enabled: false

# Native LoadBalancer service support
# Use L2 for KIND, BGP for production
loadBalancer:
  l2:
    enabled: true
    # Enable L2 announcements for KIND stability
    interfaces:
    - eth0
  algorithm: maglev
  mode: snat

# Kube-proxy replacement enabled for proper DNS and service resolution
# CRITICAL: This must be enabled for CoreDNS and service DNS resolution to work properly
# The --set flags in deploy.sh override this value to ensure it's always enabled
kubeProxyReplacement: true

# eBPF-based kube-proxy replacement features disabled
bpf:
  # Native socket load balancing disabled on Docker Desktop
  sockLB:
    enabled: false
  # Host datapath acceleration disabled on Docker Desktop
  hostLB:
    enabled: false
  # Connection tracking disabled on Docker Desktop
  ctAny:
    enabled: false
  # Program Pin Path
  mapShare: true

# Cilium Operator replicas (set to 1 for laptop deployment)
operator:
  replicas: 1

# Cluster networking
cluster:
  name: platform
  id: 1

# Security - mTLS within cluster
hubble:
  enabled: true
  relay:
    enabled: true
  ui:
    enabled: true
    backend:
      service:
        type: ClusterIP

# Observability - Prometheus metrics
prometheus:
  enabled: true
  port: 9090
  serviceMonitor:
    # Disabled initially - Prometheus Operator CRDs not installed yet
    # Will be enabled after Prometheus Operator is deployed via ArgoCD
    enabled: false
    namespace: monitoring
    interval: 30s

# Network Policy engine
# ENABLED: DNS bootstrap fixed with explicit allow rules in core-policies.yaml
# All namespaces have allow-dns policy for CoreDNS on port 53 (UDP/TCP)
# Policy enforcement will block all undefined communication (default-deny)
networkPolicy:
  enabled: true

# Policy Enforcement Mode
# Set to 'default' to enforce network policies cluster-wide
# Requires all inter-pod communication to be explicitly allowed via NetworkPolicy CRDs
policyEnforcementMode: "default"

# Pod Execution Spaces
podExecutionSpaces:
  enabled: false

# DNS Policy (optional, for DNS proxying)
dnsPolicy: ClusterFirstWithHostNet

# Resources (adjust based on cluster size)
resources:
  limits:
    cpu: 1000m
    memory: 1Gi
  requests:
    cpu: 100m
    memory: 128Mi

# Node affinity for BGP nodes (if using labels)
nodeSelector:
  kubernetes.io/os: linux

# Tolerations for node taints
tolerations:
  - operator: Exists
    effect: NoSchedule
  - operator: Exists
    effect: NoExecute

# Host networking
hostNetwork: true

# Privileged mode required for eBPF
privileged: true

# Update strategy
updateStrategy:
  type: RollingUpdate
  rollingUpdate:
    maxUnavailable: 2

# Environment variables
env:
  CILIUM_ENABLE_ENVOY_CONFIG_API: "false"
  CILIUM_ENABLE_HUBBLE_OPEN_METRICS: "true"
  CILIUM_HUBBLE_METRICS_PORT: "9091"
  # Increase Kubernetes API client timeout for Docker Desktop bootstrap
  KUBE_CLIENT_TIMEOUT: "180s"
  KUBE_CLIENT_BACKOFF_DURATION: "180"
