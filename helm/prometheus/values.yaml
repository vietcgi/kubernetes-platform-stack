# Prometheus Stack Helm Chart Values
# Observability: Prometheus (metrics), Grafana (dashboards), Loki (logs), Tempo (traces)

# Kube Prometheus Stack (includes Prometheus, Alertmanager, Node Exporter, Kube-state-metrics)
kube-prometheus-stack:
  # Prometheus
  prometheus:
    enabled: true
    prometheusSpec:
      replicas: 1
      image:
        repository: quay.io/prometheus/prometheus
        tag: v2.48.0
      resources:
        requests:
          cpu: 100m
          memory: 256Mi
        limits:
          cpu: 500m
          memory: 1Gi
      retention: 15d
      storageSpec:
        volumeClaimTemplate:
          spec:
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 10Gi
      serviceMonitorSelectorNilUsesHelmValues: false
      podMonitorSelectorNilUsesHelmValues: false
      ruleSelector: {}
      ruleSelectorNilUsesHelmValues: false

  # Grafana
  grafana:
    enabled: true
    replicas: 1
    image:
      repository: grafana/grafana
      tag: "11.0.0"
    # Admin password managed via sealed secret (grafana-admin secret)
    # For demo env: password is "demo" (sealed encrypted)
    adminPassword: ""
    existingSecret: "grafana-admin"  # Reference to sealed secret
    service:
      type: LoadBalancer
      port: 80
      targetPort: 3000
    resources:
      requests:
        cpu: 50m
        memory: 128Mi
      limits:
        cpu: 200m
        memory: 512Mi
    # Datasources for Prometheus, Loki, Tempo
    datasources:
      datasources.yaml:
        apiVersion: 1
        datasources:
        - name: Prometheus
          type: prometheus
          url: http://kube-prometheus-stack-prometheus:9090
          isDefault: true
        - name: Loki
          type: loki
          url: http://loki:3100
        - name: Tempo
          type: tempo
          url: http://tempo:3100
    # Dashboard sidecar for auto-discovery
    sidecar:
      dashboards:
        enabled: true
        label: grafana_dashboard
        labelValue: "1"
        searchNamespace: monitoring
        folder: /tmp/dashboards
        provider:
          foldersFromFilesStructure: true

  # Prometheus Operator
  prometheusOperator:
    enabled: true
    resources:
      requests:
        cpu: 50m
        memory: 128Mi
      limits:
        cpu: 200m
        memory: 256Mi

  # Node Exporter
  nodeExporter:
    enabled: true
    resources:
      requests:
        cpu: 50m
        memory: 64Mi
      limits:
        cpu: 100m
        memory: 128Mi

  # Kube State Metrics
  kubeStateMetrics:
    enabled: true
    resources:
      requests:
        cpu: 50m
        memory: 64Mi
      limits:
        cpu: 100m
        memory: 256Mi

  # Alertmanager
  alertmanager:
    enabled: true
    alertmanagerSpec:
      replicas: 1
      storage:
        volumeClaimTemplate:
          spec:
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 1Gi

# Loki Stack (Log Aggregation)
loki-stack:
  enabled: true
  loki:
    enabled: true
    image:
      repository: grafana/loki
      tag: "3.0.0"
    persistence:
      enabled: true
      size: 10Gi
    resources:
      requests:
        cpu: 50m
        memory: 128Mi
      limits:
        cpu: 200m
        memory: 512Mi
    config:
      auth_enabled: false
      ingester:
        chunk_idle_period: 3m
        max_chunk_age: 1h
      limits_config:
        enforce_metric_name: false
        max_entries_limit_per_second: 20000

  # Promtail (Log shipper)
  promtail:
    enabled: true
    image:
      repository: grafana/promtail
      tag: "3.0.0"
    resources:
      requests:
        cpu: 50m
        memory: 64Mi
      limits:
        cpu: 100m
        memory: 256Mi

# Tempo (Distributed Tracing)
tempo:
  enabled: true
  image:
    repository: grafana/tempo
    tag: "2.3.0"
  replicas: 1
  persistence:
    enabled: true
    size: 5Gi
  resources:
    requests:
      cpu: 50m
      memory: 128Mi
    limits:
      cpu: 200m
      memory: 512Mi
  config:
    metrics_generator_enabled: true
    metrics_generator:
      collection_interval: 15s
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318
      jaeger:
        protocols:
          grpc:
            endpoint: 0.0.0.0:14250
          thrift_http:
            endpoint: 0.0.0.0:14268

# Service Monitor for custom app metrics
serviceMonitor:
  enabled: true
  # Example: Monitor my-app service
  # labels:
  #   release: prometheus
  # endpoints:
  # - interval: 30s
  #   path: /metrics
  #   port: metrics

# PrometheusRule for alerting
prometheusRule:
  enabled: true
  # Define custom alert rules here
  groups:
    # Kubernetes cluster health alerts
    - name: kubernetes.rules
      interval: 30s
      rules:
      # Node alerts
      - alert: NodeNotReady
        expr: 'kube_node_status_condition{condition="Ready",status="true"} == 0'
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Node {{ $labels.node }} is not ready"
          description: "Node {{ $labels.node }} has been in NotReady state for more than 5 minutes"

      - alert: NodeMemoryPressure
        expr: 'kube_node_status_condition{condition="MemoryPressure",status="true"} == 1'
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Node {{ $labels.node }} has MemoryPressure"
          description: "Node {{ $labels.node }} has MemoryPressure condition"

      - alert: NodeDiskPressure
        expr: 'kube_node_status_condition{condition="DiskPressure",status="true"} == 1'
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Node {{ $labels.node }} has DiskPressure"
          description: "Node {{ $labels.node }} has DiskPressure condition"

      # Pod restart alerts
      - alert: PodCrashLooping
        expr: 'rate(kube_pod_container_status_restarts_total[15m]) > 0'
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping"
          description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} container {{ $labels.container }} has restarted {{ $value | humanize }} times in the last 15 minutes"

      - alert: PodNotHealthy
        expr: 'min_over_time(sum by (namespace, pod) (kube_pod_status_phase{phase=~"Pending|Unknown|Failed"})[15m:1m]) > 0'
        for: 15m
        labels:
          severity: critical
        annotations:
          summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is not healthy"
          description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready state for more than 15 minutes"

      # Container resource alerts
      - alert: ContainerCpuUsageHigh
        expr: '(sum(rate(container_cpu_usage_seconds_total{container!=""}[5m])) by (namespace, pod, container) / on (namespace, pod, container) kube_pod_container_resource_limits{resource="cpu"}) > 0.9'
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Container {{ $labels.namespace }}/{{ $labels.pod }}/{{ $labels.container }} CPU usage is high"
          description: "Container CPU usage is {{ $value | humanizePercentage }} of its limit"

      - alert: ContainerMemoryUsageHigh
        expr: '(sum(container_memory_working_set_bytes{container!=""}) by (namespace, pod, container) / on (namespace, pod, container) kube_pod_container_resource_limits{resource="memory"}) > 0.9'
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Container {{ $labels.namespace }}/{{ $labels.pod }}/{{ $labels.container }} memory usage is high"
          description: "Container memory usage is {{ $value | humanizePercentage }} of its limit"

    # Persistent volume alerts
    - name: persistent_volume.rules
      interval: 30s
      rules:
      - alert: PersistentVolumeUsageHigh
        expr: '(kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes) > 0.85'
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "PersistentVolume {{ $labels.persistentvolumeclaim }} usage is high"
          description: "PersistentVolume {{ $labels.persistentvolumeclaim }} in {{ $labels.namespace }} is {{ $value | humanizePercentage }} full"

      - alert: PersistentVolumeCritical
        expr: '(kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes) > 0.95'
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "PersistentVolume {{ $labels.persistentvolumeclaim }} is nearly full"
          description: "PersistentVolume {{ $labels.persistentvolumeclaim }} in {{ $labels.namespace }} is {{ $value | humanizePercentage }} full"

      - alert: PersistentVolumeInodesFull
        expr: '(kubelet_volume_stats_inodes_used / kubelet_volume_stats_inodes) > 0.95'
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "PersistentVolume {{ $labels.persistentvolumeclaim }} inodes are nearly full"
          description: "PersistentVolume {{ $labels.persistentvolumeclaim }} in {{ $labels.namespace }} has {{ $value | humanizePercentage }} inodes used"

    # Deployment and StatefulSet alerts
    - name: deployment.rules
      interval: 30s
      rules:
      - alert: DeploymentReplicasMismatch
        expr: 'kube_deployment_spec_replicas{} != kube_deployment_status_replicas_updated{}'
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} replicas mismatch"
          description: "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has not matched the expected number of replicas for more than 10 minutes"

      - alert: StatefulsetReplicasMismatch
        expr: 'kube_statefulset_status_replicas_ready{} != kube_statefulset_status_replicas{}'
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} replicas mismatch"
          description: "StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} has not matched the expected number of replicas"

    # Network and service alerts
    - name: network.rules
      interval: 30s
      rules:
      - alert: KubernetesApiServerLatency
        expr: 'histogram_quantile(0.99, sum(rate(apiserver_request_duration_seconds_bucket[5m])) by (verb, le)) > 1'
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Kubernetes API server latency is high"
          description: "99th percentile latency for {{ $labels.verb }} requests is {{ $value }}s"

      - alert: KubernetesApiServerErrors
        expr: 'sum(rate(apiserver_request_total{code=~"5.."}[5m])) by (code) > 0.1'
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Kubernetes API server error rate is high"
          description: "API server error rate is {{ $value | humanize }} requests/sec"

    # etcd alerts
    - name: etcd.rules
      interval: 30s
      rules:
      - alert: EtcdLeaderChanged
        expr: 'delta(etcd_server_has_leader[10m]) > 1'
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "etcd leader has changed"
          description: "etcd leader changed {{ $value }} times in the last 10 minutes"

      - alert: EtcdBackendCommitDurationHigh
        expr: 'histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket[5m])) > 0.25'
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "etcd commit duration is high"
          description: "etcd commit duration is {{ $value }}s"

    # Prometheus alerts
    - name: prometheus.rules
      interval: 30s
      rules:
      - alert: PrometheusConfigReloadFailed
        expr: 'prometheus_config_last_reload_successful == 0'
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "Prometheus config reload failed"
          description: "Prometheus config reload has failed"

      - alert: PrometheusTargetDown
        expr: 'up == 0'
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Prometheus target {{ $labels.instance }} is down"
          description: "Target {{ $labels.instance }} has been down for more than 5 minutes"

      - alert: PrometheusRuleEvaluationFailures
        expr: 'increase(prometheus_rule_evaluation_failures_total[15m]) > 0'
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Prometheus rule evaluation failures"
          description: "Prometheus has {{ $value }} rule evaluation failures in the last 15 minutes"

      - alert: PrometheusHighMemoryUsage
        expr: '(process_resident_memory_bytes / process_virtual_memory_max_bytes) > 0.9'
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Prometheus high memory usage"
          description: "Prometheus memory usage is {{ $value | humanizePercentage }} of its limit"

# Namespace for observability components
namespace: monitoring

# RBAC
rbac:
  create: true

# Service accounts
serviceAccount:
  create: true
  name: observability
