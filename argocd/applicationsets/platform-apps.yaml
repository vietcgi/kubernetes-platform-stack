# Enterprise ApplicationSet Template
# Generates all platform applications from a single source of truth
# Uses Helm repositories as the source for all applications
apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: platform-applications
  namespace: argocd
  labels:
    app.kubernetes.io/name: platform-applicationset
    app.kubernetes.io/part-of: argocd
spec:
  goTemplate: true
  generators:
  # List-based generator using shared app configuration
  - list:
      elements:
      # CNI - Must be first (foundational networking)
      - name: cilium
        namespace: kube-system
        repoURL: https://helm.cilium.io
        chart: cilium
        version: "1.18.4"
        syncPolicy: conservative-bootstrap
        group: infrastructure
        syncWave: "-5"
        tier: infrastructure
        criticality: critical

      # DNS - Critical for cluster resolution
      - name: coredns
        namespace: kube-system
        repoURL: https://coredns.github.io/helm
        chart: coredns
        version: "1.45.0"
        syncPolicy: conservative-bootstrap
        group: infrastructure
        syncWave: "-5"
        tier: infrastructure
        criticality: critical

      # Observability Stack
      - name: prometheus
        namespace: monitoring
        repoURL: https://prometheus-community.github.io/helm-charts
        chart: kube-prometheus-stack
        version: "79.5.0"
        syncPolicy: aggressive
        group: observability
        syncWave: "5"
        tier: observability
        criticality: high

      - name: metrics-server
        namespace: kube-system
        repoURL: https://kubernetes-sigs.github.io/metrics-server/
        chart: metrics-server
        version: "3.13.0"
        syncPolicy: aggressive
        group: infrastructure
        syncWave: "5"
        tier: infrastructure
        criticality: high

      - name: loki
        namespace: monitoring
        repoURL: https://grafana.github.io/helm-charts
        chart: loki-stack
        version: "2.10.3"
        syncPolicy: aggressive
        group: observability
        syncWave: "5"
        tier: observability
        criticality: high

      - name: tempo
        namespace: monitoring
        repoURL: https://grafana.github.io/helm-charts
        chart: tempo
        version: "1.24.0"
        syncPolicy: aggressive
        group: observability
        syncWave: "5"
        tier: observability
        criticality: medium

      # Service Mesh
      - name: istio
        namespace: istio-system
        repoURL: https://istio-release.storage.googleapis.com/charts
        chart: istiod
        version: "1.28.0"
        syncPolicy: conservative
        group: service_mesh
        syncWave: "0"
        tier: service_mesh
        criticality: medium

      # Security Layer
      - name: cert-manager
        namespace: cert-manager
        repoURL: https://charts.jetstack.io
        chart: cert-manager
        version: "v1.19.1"
        syncPolicy: aggressive
        group: security
        syncWave: "-1"
        tier: infrastructure
        criticality: high

      - name: vault
        namespace: vault
        repoURL: https://helm.releases.hashicorp.com
        chart: vault
        version: "0.31.0"
        syncPolicy: aggressive
        group: security
        syncWave: "-1"
        tier: security
        criticality: high

      - name: falco
        namespace: falco
        repoURL: https://falcosecurity.github.io/charts
        chart: falco
        version: "7.0.1"
        syncPolicy: aggressive
        group: security
        syncWave: "-3"
        tier: security
        criticality: high

      - name: kyverno
        namespace: kyverno
        repoURL: https://kyverno.github.io/kyverno
        chart: kyverno
        version: "3.6.0"
        syncPolicy: aggressive
        group: security
        syncWave: "-3"
        tier: security
        criticality: critical

      - name: sealed-secrets
        namespace: sealed-secrets
        repoURL: https://bitnami-labs.github.io/sealed-secrets
        chart: sealed-secrets
        version: "2.17.9"
        syncPolicy: aggressive
        group: security
        syncWave: "-1"
        tier: security
        criticality: high

      - name: external-secrets-operator
        namespace: external-secrets
        repoURL: https://charts.external-secrets.io
        chart: external-secrets
        version: "0.14.0"
        syncPolicy: aggressive
        group: security
        syncWave: "-1"
        tier: infrastructure
        criticality: high

      # Governance Layer
      - name: gatekeeper
        namespace: gatekeeper-system
        repoURL: https://open-policy-agent.github.io/gatekeeper/charts
        chart: gatekeeper
        version: "3.20.1"
        syncPolicy: aggressive
        group: governance
        syncWave: "-3"
        tier: security
        criticality: critical

      # Enterprise Features - Ingress & API Management
      - name: external-dns
        namespace: external-dns
        repoURL: https://kubernetes-sigs.github.io/external-dns/
        chart: external-dns
        version: "1.19.0"
        syncPolicy: aggressive
        group: infrastructure
        syncWave: "0"
        tier: infrastructure
        criticality: high

      - name: kong
        namespace: api-gateway
        repoURL: https://charts.konghq.com
        chart: kong
        version: "2.52.0"
        syncPolicy: aggressive
        group: infrastructure
        syncWave: "0"
        tier: platform
        criticality: medium

      # Enterprise Features - Storage & Backup
      - name: longhorn
        namespace: longhorn-system
        repoURL: https://charts.longhorn.io
        chart: longhorn
        version: "1.10.1"
        syncPolicy: aggressive
        group: infrastructure
        syncWave: "10"
        tier: storage
        criticality: high

      - name: velero
        namespace: velero
        repoURL: https://vmware-tanzu.github.io/helm-charts
        chart: velero
        version: "11.1.1"
        syncPolicy: aggressive
        group: infrastructure
        syncWave: "10"
        tier: storage
        criticality: high

      # Enterprise Features - Advanced Observability
      - name: jaeger
        namespace: tracing
        repoURL: https://jaegertracing.github.io/helm-charts
        chart: jaeger
        version: "3.4.1"
        syncPolicy: aggressive
        group: observability
        syncWave: "5"
        tier: observability
        criticality: medium

      # Synthetic Monitoring - Endpoint Availability
      - name: blackbox-exporter
        namespace: monitoring
        repoURL: https://prometheus-community.github.io/helm-charts
        chart: prometheus-blackbox-exporter
        version: "11.4.2"
        syncPolicy: aggressive
        group: observability
        syncWave: "5"
        tier: observability
        criticality: medium

      # Enterprise Features - Container Registry
      - name: harbor
        namespace: harbor
        repoURL: https://helm.goharbor.io
        chart: harbor
        version: "1.18.0"
        syncPolicy: aggressive
        group: infrastructure
        syncWave: "15"
        tier: platform
        criticality: medium

  template:
    metadata:
      name: '{{ .name }}'
      labels:
        app.kubernetes.io/name: '{{ .name }}'
        app.kubernetes.io/group: '{{ .group }}'
        app.kubernetes.io/managed-by: applicationset
        app.kubernetes.io/part-of: platform
        tier: '{{ .tier }}'
        criticality: '{{ .criticality }}'
      finalizers:
        - resources-finalizer.argocd.argoproj.io
    spec:
      project: default
      source:
        repoURL: '{{ .repoURL }}'
        chart: '{{ .chart }}'
        targetRevision: '{{ .version }}'
        helm:
          releaseName: '{{ .name }}'
          values: |
            {{- if not (or (eq .name "cert-manager") (eq .name "falco") (eq .name "gatekeeper") (eq .name "sealed-secrets") (eq .name "istio") (eq .name "kyverno") (eq .name "tempo") (eq .name "velero") (eq .name "cilium")) }}
            # Default minimal replicas for laptop deployment
            replicaCount: 1
            replicas: 1
            {{- end }}
            {{ if eq .name "coredns" }}
            # CoreDNS - Critical DNS infrastructure
            # Configuration must match helm/coredns/values.yaml used during bootstrap
            service:
              name: kube-dns
              clusterIP: 10.96.0.10
            resources:
              limits:
                cpu: 100m
                memory: 128Mi
              requests:
                cpu: 50m
                memory: 64Mi
            replicaCount: 2
            servers:
            - zones:
              - zone: .
              port: 53
              plugins:
              - name: errors
              - name: health
                configBlock: |-
                  lameduck 5s
              - name: ready
              - name: kubernetes
                parameters: cluster.local in-addr.arpa ip6.arpa
                configBlock: |-
                  pods insecure
                  fallthrough in-addr.arpa ip6.arpa
                  ttl 30
              - name: prometheus
                parameters: 0.0.0.0:9153
              - name: forward
                parameters: . /etc/resolv.conf
              - name: cache
                parameters: 30
              - name: loop
              - name: reload
              - name: loadbalance
            serviceMonitor:
              enabled: false
            podDisruptionBudget:
              minAvailable: 1
            tolerations:
            - key: node.kubernetes.io/not-ready
              operator: Exists
              effect: NoSchedule
            - key: node-role.kubernetes.io/control-plane
              operator: Exists
              effect: NoSchedule
            - key: CriticalAddonsOnly
              operator: Exists
              effect: NoExecute
            podLabels:
              app.kubernetes.io/instance: coredns
              app.kubernetes.io/name: coredns
              k8s-app: kube-dns
            {{ else if eq .name "cilium" }}
            # Cilium CNI - Critical networking infrastructure
            image:
              repository: quay.io/cilium/cilium
              tag: v1.18.4
              pullPolicy: IfNotPresent
            ipam:
              mode: kubernetes
            bgp:
              enabled: true
              announce:
                loadbalancerIP: true
                podCIDR: true
            bgpControlPlane:
              enabled: true
              secretsNamespace:
                create: false
                name: kube-system
            bgpClusterConfig:
              enabled: false
              nodeSelector:
                bgp-node: "true"
              localASN: 65000
              exportPodCIDR: true
              routerID: "192.168.1.1"
            bgpNeighbors:
              enabled: false
            bgpAdvertisements:
              enabled: false
            loadBalancer:
              l2:
                enabled: true
                interfaces:
                - eth0
              algorithm: maglev
              mode: snat
            kubeProxyReplacement: true
            # k8sServiceHost/Port set dynamically by deploy.sh during bootstrap
            # ArgoCD does not manage these values to keep config portable
            bpf:
              sockLB:
                enabled: false
              hostLB:
                enabled: false
              ctAny:
                enabled: false
              mapShare: true
            operator:
              replicas: 1
            cluster:
              name: platform
              id: 1
            hubble:
              enabled: true
              relay:
                enabled: true
              ui:
                enabled: true
                backend:
                  service:
                    type: ClusterIP
            prometheus:
              enabled: true
              port: 9090
              serviceMonitor:
                enabled: false
                namespace: monitoring
                interval: 30s
            networkPolicy:
              enabled: true
            policyEnforcementMode: "default"
            podExecutionSpaces:
              enabled: false
            dnsPolicy: ClusterFirstWithHostNet
            resources:
              limits:
                cpu: 1000m
                memory: 1Gi
              requests:
                cpu: 100m
                memory: 128Mi
            nodeSelector:
              kubernetes.io/os: linux
            tolerations:
              - operator: Exists
                effect: NoSchedule
              - operator: Exists
                effect: NoExecute
            hostNetwork: true
            privileged: true
            updateStrategy:
              type: RollingUpdate
              rollingUpdate:
                maxUnavailable: 2
            env:
              CILIUM_ENABLE_ENVOY_CONFIG_API: "false"
              CILIUM_ENABLE_HUBBLE_OPEN_METRICS: "true"
              CILIUM_HUBBLE_METRICS_PORT: "9091"
              KUBE_CLIENT_TIMEOUT: "180s"
              KUBE_CLIENT_BACKOFF_DURATION: "180"
            {{ else if eq .name "cert-manager" }}
            # Enable CRD installation for cert-manager
            crds:
              enabled: true
            # cert-manager specific replica settings
            replicaCount: 1
            webhook:
              replicaCount: 1
            cainjector:
              replicaCount: 1
            {{ else if eq .name "falco" }}
            # Falco runtime security with modern eBPF driver for KIND/Docker Desktop
            replicaCount: 1
            # Use modern eBPF driver (doesn't need kernel headers)
            driver:
              kind: modern_ebpf
            # Falco is a runtime security event listener, not an HTTP UI service
            # It exposes events via gRPC and event streams, not HTTP
            webui:
              enabled: false
            {{ else if eq .name "gatekeeper" }}
            # Gatekeeper policy enforcement (OPA-based policy engine)
            replicas: 1
            audit:
              replicas: 1
            # Disable CRD upgrades (let operator manage them)
            disableMutation: false
            disableValidatingWebhook: false
            enableExternalData: true
            # Gatekeeper is a webhook-based policy engine, not an HTTP UI service
            auditViewer:
              enabled: false
            {{ else if eq .name "sealed-secrets" }}
            # Sealed Secrets
            replicaCount: 1
            {{ else if eq .name "prometheus" }}
            # Kube-prometheus-stack: Using v70.10.0 to avoid CRD annotation size issues
            # Disable CRD installation - CRDs are managed separately via prometheus-crds Application
            crds:
              enabled: false
            prometheus:
              prometheusSpec:
                podMonitorNamespaceSelector: {}
                replicas: 1
              # Prometheus ingress for Kong
              ingress:
                enabled: true
                ingressClassName: kong
                annotations:
                  konghq.com/strip-path: "true"
                hosts:
                  - prometheus.demo.local
                paths:
                  - /
            # Grafana ingress for Kong (part of kube-prometheus-stack)
            grafana:
              replicas: 1
              ingress:
                enabled: true
                ingressClassName: kong
                annotations:
                  konghq.com/strip-path: "true"
                hosts:
                  - grafana.demo.local
                path: /
              # Admin password should be managed via Sealed Secrets, not hardcoded
              adminPassword: ""
              # Additional datasources - Loki (NOT default, Prometheus is default)
              additionalDataSources:
              - name: Loki
                type: loki
                access: proxy
                url: http://loki:3100
                version: 1
                isDefault: false
                jsonData: {}
            kubeStateMetrics:
              enabled: true
              replicas: 1
            nodeExporter:
              enabled: true
            alertmanager:
              enabled: true
              alertmanagerSpec:
                replicas: 1
            {{ else if eq .name "external-dns" }}
            # External DNS for automatic DNS record management
            policy: sync
            registry: txt
            txtOwnerId: external-dns
            {{ else if eq .name "metrics-server" }}
            # Metrics-server for KIND - disable TLS verification for kubelet certificates
            # KIND kubelet certificates don't include IP SANs, so verification fails
            args:
              - --kubelet-insecure-tls
              - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
            {{ else if eq .name "loki" }}
            # Loki backend logging service (accessed via Grafana datasources, not HTTP UI)
            loki:
              ingress:
                enabled: false
            # Disable Grafana and its datasource sidecar (datasource managed in prometheus/grafana)
            grafana:
              enabled: false
              sidecar:
                datasources:
                  enabled: false
            promtail:
              config:
                clients:
                  - url: http://loki:3100/loki/api/v1/push
            {{ else if eq .name "istio" }}
            # Istio service mesh
            pilot:
              replicaCount: 1
            {{ else if eq .name "kyverno" }}
            # Kyverno policy engine (webhook-based policy validation)
            replicaCount: 1
            admissionController:
              replicas: 1
            backgroundController:
              replicas: 1
            cleanupController:
              replicas: 1
            reportsController:
              replicas: 1
            # Fix kubectl image for cleanup jobs
            cleanupJobs:
              admissionReports:
                image:
                  registry: docker.io
                  repository: bitnami/kubectl
                  tag: "1.31"
              clusterAdmissionReports:
                image:
                  registry: docker.io
                  repository: bitnami/kubectl
                  tag: "1.31"
            # Kyverno is a webhook-based policy validator, not an HTTP UI service
            reportsUI:
              enabled: false
            {{ else if eq .name "tempo" }}
            # Tempo distributed tracing backend (accessed via Grafana datasources, not HTTP UI)
            replicas: 1
            ingress:
              enabled: false
            {{ else if eq .name "jaeger" }}
            # Jaeger distributed tracing with in-memory storage (suitable for KIND)
            provisionDataStore:
              cassandra: false
              elasticsearch: false
            storage:
              type: memory
            collector:
              service:
                type: LoadBalancer
            query:
              service:
                type: LoadBalancer
              # Jaeger query ingress for Kong
              ingress:
                enabled: true
                ingressClassName: kong
                annotations:
                  konghq.com/strip-path: "true"
                hosts:
                  - jaeger.demo.local
            {{ else if eq .name "kong" }}
            # Kong API Gateway with LoadBalancer and hostNetwork for KIND port access
            admin:
              enabled: true
              service:
                type: ClusterIP
            proxy:
              service:
                type: LoadBalancer
              containerPort: 8000
            # Use hostNetwork to make Kong accessible on host ports (80/443)
            podSecurityPolicy:
              enabled: false
            deployment:
              hostNetwork: true
              replicas: 1
            # Enable Kong Ingress Controller for ingress routing
            ingressController:
              enabled: true
              installCRDs: true
            {{ else if eq .name "harbor" }}
            # Harbor container registry
            # Default admin password managed via sealed secret (harbor-admin secret)
            # For demo env: password is "demo" (sealed encrypted)
            persistence:
              enabled: true
              persistentVolumeClaim:
                registry:
                  size: 5Gi
            # Harbor database probe configuration (increase timeouts for PostgreSQL startup)
            database:
              livenessProbe:
                timeoutSeconds: 5
                initialDelaySeconds: 30
              readinessProbe:
                timeoutSeconds: 5
                initialDelaySeconds: 15
            # Harbor ingress for Kong
            expose:
              type: ingress
              ingress:
                className: kong
                annotations:
                  konghq.com/strip-path: "true"
                hosts:
                  core: harbor.demo.local
                  notary: harbor-notary.demo.local
            {{ else if eq .name "longhorn" }}
            # Longhorn storage - disable hooks due to ServiceAccount dependency issue
            # The pre-upgrade hook requires ServiceAccount that's created in Sync phase
            preUpgradeChecker:
              jobEnabled: false
            postUpgradeChecker:
              jobEnabled: false
            {{ else if eq .name "vault" }}
            # Vault with enabled ingress and Kyverno-compliant security settings
            ui:
              enabled: true
            server:
              # Security context and resource limits required by Kyverno policies
              resources:
                limits:
                  cpu: 500m
                  memory: 512Mi
                requests:
                  cpu: 250m
                  memory: 256Mi
              securityContext:
                runAsNonRoot: false  # Vault requires root for certain operations
                runAsUser: 100       # Run as vault user (UID 100)
                fsGroup: 1000
                capabilities:
                  drop:
                    - ALL
                  add:
                    - IPC_LOCK  # Vault needs this capability for memory locking
                readOnlyRootFilesystem: false  # Vault needs to write to /vault/data
              # Pod-level security context
              podSecurityContext:
                runAsNonRoot: false
                runAsUser: 100
                fsGroup: 1000
              ingress:
                enabled: true
                ingressClassName: kong
                annotations:
                  konghq.com/strip-path: "true"
                hosts:
                  - host: vault.demo.local
                    paths: [/]
            {{ else if eq .name "velero" }}
            # Velero backup solution - disable CRD upgrade for fresh install
            upgradeCRDs: false
            kubectl:
              image:
                repository: docker.io/bitnami/kubectl
                tag: "1.31"
            configuration:
              backupStorageLocation:
                - name: default
                  provider: aws
                  bucket: velero-backups
                  config:
                    region: us-east-1
              volumeSnapshotLocation:
                - name: default
                  provider: aws
                  config:
                    region: us-east-1
            initContainers:
              - name: velero-plugin-for-aws
                image: velero/velero-plugin-for-aws:v1.9.0
                volumeMounts:
                  - mountPath: /target
                    name: plugins
            {{ end }}
      destination:
        server: https://kubernetes.default.svc
        namespace: '{{ .namespace }}'
      syncPolicy:
        automated:
          selfHeal: true
        syncOptions:
          - CreateNamespace=true
          - RespectIgnoreDifferences=true
          - SkipDryRunOnMissingResource=true
          - ServerSideApply=true
        managedNamespaceMetadata:
          labels: {}
          annotations: {}
        retry:
          limit: 10
          backoff:
            duration: 5s
            factor: 2
            maxDuration: 5m
      ignoreDifferences:
        - group: apiextensions.k8s.io
          kind: CustomResourceDefinition
          jsonPointers:
            - /spec/conversion
            - /status
            - /metadata/annotations
        - group: batch
          kind: Job
          name: velero-upgrade-crds
          namespace: velero
        # Velero VolumeSnapshotLocation (ignore validation errors without cloud provider)
        - group: velero.io
          kind: VolumeSnapshotLocation
          name: default
          namespace: velero
          jsonPointers:
            - /spec
        # Ingress resources (ignore LoadBalancer status in local environments)
        - group: networking.k8s.io
          kind: Ingress
          name: loki
          namespace: monitoring
          jsonPointers:
            - /status/loadBalancer
        - group: networking.k8s.io
          kind: Ingress
          name: vault
          namespace: vault
          jsonPointers:
            - /status/loadBalancer
        - group: networking.k8s.io
          kind: Ingress
          name: prometheus-grafana
          namespace: monitoring
          jsonPointers:
            - /status/loadBalancer
        - group: networking.k8s.io
          kind: Ingress
          name: prometheus-kube-prometheus-prometheus
          namespace: monitoring
          jsonPointers:
            - /status/loadBalancer
        # Cilium DaemonSet (ignore appArmorProfile schema field not in CRD)
        - group: apps
          kind: DaemonSet
          name: cilium
          namespace: kube-system
          jsonPointers:
            - /spec/template/spec/securityContext/appArmorProfile
        # Vault StatefulSet (ignore dynamic fields)
        - group: apps
          kind: StatefulSet
          name: vault
          namespace: vault
          jsonPointers:
            - /spec/volumeClaimTemplates
        # Istio webhook mutations (modified by istiod after startup, ignore ALL changes)
        - group: admissionregistration.k8s.io
          kind: MutatingWebhookConfiguration
          name: istio-sidecar-injector
          jsonPointers:
            - /webhooks
            - /metadata/annotations
            - /metadata/labels
        - group: admissionregistration.k8s.io
          kind: ValidatingWebhookConfiguration
          name: istio-validator-istio-system
          jsonPointers:
            - /webhooks
            - /metadata/annotations
            - /metadata/labels
        # Velero Deployment (modified by controller, ignore spec and metadata changes)
        - group: apps
          kind: Deployment
          name: velero
          namespace: velero
          jsonPointers:
            - /spec
            - /metadata/annotations
            - /metadata/labels
        # Harbor StatefulSets (ignore dynamic fields updated by controller)
        - group: apps
          kind: StatefulSet
          namespace: harbor
          jsonPointers:
            - /spec/volumeClaimTemplates
            - /spec/template/metadata/annotations
            - /spec/replicas
        # Istio Deployment and HPA (modified by istiod)
        - group: apps
          kind: Deployment
          name: istiod
          namespace: istio-system
          jsonPointers:
            - /spec/template/metadata/annotations
            - /metadata/annotations
        - group: autoscaling
          kind: HorizontalPodAutoscaler
          name: istiod
          namespace: istio-system
          jsonPointers:
            - /spec
            - /status
        # Kyverno webhook mutations (rules auto-generated by admission webhook)
        - group: kyverno.io
          kind: ClusterPolicy
          jqPathExpressions:
            - '.spec.rules[] | select(.name | startswith("autogen-"))'
            - '.status'
        # Gatekeeper CRDs (auto-managed, ignore spec and metadata changes)
        - group: apiextensions.k8s.io
          kind: CustomResourceDefinition
          name: assign.mutations.gatekeeper.sh
          jsonPointers:
            - /spec
            - /metadata/annotations
            - /metadata/labels
        - group: apiextensions.k8s.io
          kind: CustomResourceDefinition
          name: assignimage.mutations.gatekeeper.sh
          jsonPointers:
            - /spec
            - /metadata/annotations
            - /metadata/labels
        - group: apiextensions.k8s.io
          kind: CustomResourceDefinition
          name: assignmetadata.mutations.gatekeeper.sh
          jsonPointers:
            - /spec
            - /metadata/annotations
            - /metadata/labels
        - group: apiextensions.k8s.io
          kind: CustomResourceDefinition
          name: configpodstatuses.status.gatekeeper.sh
          jsonPointers:
            - /spec
            - /metadata/annotations
            - /metadata/labels
        - group: apiextensions.k8s.io
          kind: CustomResourceDefinition
          name: configs.config.gatekeeper.sh
          jsonPointers:
            - /spec
            - /metadata/annotations
            - /metadata/labels
        - group: apiextensions.k8s.io
          kind: CustomResourceDefinition
          name: connectionpodstatuses.status.gatekeeper.sh
          jsonPointers:
            - /spec
            - /metadata/annotations
            - /metadata/labels
        - group: apiextensions.k8s.io
          kind: CustomResourceDefinition
          name: connections.connection.gatekeeper.sh
          jsonPointers:
            - /spec
            - /metadata/annotations
            - /metadata/labels
        - group: apiextensions.k8s.io
          kind: CustomResourceDefinition
          name: constraintpodstatuses.status.gatekeeper.sh
          jsonPointers:
            - /spec
            - /metadata/annotations
            - /metadata/labels
        - group: apiextensions.k8s.io
          kind: CustomResourceDefinition
          name: constrainttemplatepodstatuses.status.gatekeeper.sh
          jsonPointers:
            - /spec
            - /metadata/annotations
            - /metadata/labels
        - group: apiextensions.k8s.io
          kind: CustomResourceDefinition
          name: constrainttemplates.templates.gatekeeper.sh
          jsonPointers:
            - /spec
            - /metadata/annotations
            - /metadata/labels
        - group: apiextensions.k8s.io
          kind: CustomResourceDefinition
          name: expansiontemplate.expansion.gatekeeper.sh
          jsonPointers:
            - /spec
            - /metadata/annotations
            - /metadata/labels
        - group: apiextensions.k8s.io
          kind: CustomResourceDefinition
          name: expansiontemplatepodstatuses.status.gatekeeper.sh
          jsonPointers:
            - /spec
            - /metadata/annotations
            - /metadata/labels
        - group: apiextensions.k8s.io
          kind: CustomResourceDefinition
          name: modifyset.mutations.gatekeeper.sh
          jsonPointers:
            - /spec
            - /metadata/annotations
            - /metadata/labels
        - group: apiextensions.k8s.io
          kind: CustomResourceDefinition
          name: mutatorpodstatuses.status.gatekeeper.sh
          jsonPointers:
            - /spec
            - /metadata/annotations
            - /metadata/labels
        - group: apiextensions.k8s.io
          kind: CustomResourceDefinition
          name: providers.externaldata.gatekeeper.sh
          jsonPointers:
            - /spec
            - /metadata/annotations
            - /metadata/labels
        - group: apiextensions.k8s.io
          kind: CustomResourceDefinition
          name: syncsets.syncset.gatekeeper.sh
          jsonPointers:
            - /spec
            - /metadata/annotations
            - /metadata/labels
        # Kyverno CRDs (auto-managed, ignore spec and metadata changes)
        - group: apiextensions.k8s.io
          kind: CustomResourceDefinition
          name: clusterpolicies.kyverno.io
          jsonPointers:
            - /spec
            - /metadata/annotations
            - /metadata/labels
        - group: apiextensions.k8s.io
          kind: CustomResourceDefinition
          name: policies.kyverno.io
          jsonPointers:
            - /spec
            - /metadata/annotations
            - /metadata/labels
        # CoreDNS Deployment (immutable selector field set at creation, ignore selector changes)
        - group: apps
          kind: Deployment
          name: coredns
          namespace: kube-system
          jsonPointers:
            - /spec/selector
  goTemplateOptions: ["missingkey=error"]
  templatePatch: |
    metadata:
      annotations:
        {{- if .syncWave }}
        argocd.argoproj.io/sync-wave: "{{ .syncWave }}"
        {{- end }}
    spec:
      {{- if eq .syncPolicy "conservative-bootstrap" }}
      syncPolicy:
        syncOptions:
          - RespectIgnoreDifferences=true
          - ServerSideApply=true
        automated:
          prune: false
          selfHeal: true
      {{- else }}
      syncPolicy:
        automated:
          prune: {{ if eq .syncPolicy "aggressive" }}true{{ else }}false{{ end }}
      {{- end }}
      # ignoreDifferences are already defined in template.spec, but we can't merge them here in templatePatch
      # The ignoreDifferences rules will apply to all generated applications

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: applicationset-defaults
  namespace: argocd
  labels:
    app.kubernetes.io/name: applicationset-defaults
data:
  # Platform applications inventory
  platform-apps: |
    infrastructure:
      - cilium
      - metrics-server
      - external-dns
      - kong
      - longhorn
      - velero
      - harbor
    observability:
      - prometheus
      - loki
      - tempo
      - jaeger
    service_mesh:
      - istio
    security:
      - cert-manager
      - vault
      - falco
      - kyverno
      - sealed-secrets
      - external-secrets-operator
    governance:
      - gatekeeper
